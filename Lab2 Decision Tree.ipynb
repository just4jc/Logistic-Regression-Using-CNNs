{
    "nbformat_minor": 2, 
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "## Decision Tree classifier implementation in Python with sklearn Library\n\nhttp://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Import Libraries\n<p>In this tutorial, we are going to use <b>*numpy*</b>, <b>*pandas*</b> and <b>*sklearn*</b>. And the first step, of couse, is to import these libraries.</p>\n<p>\n<div class=\"btn-group\">\n  <a class=\"btn btn-info\" href=\"http://www.numpy.org\" style=\"border-radius: 2px\">numpy</button>\n  <a class=\"btn btn-info\" href=\"http://pandas.pydata.org\" style=\"border-radius: 2px\">pandas</button>\n  <a class=\"btn btn-info\" href=\"http://scikit-learn.org/stable/\" style=\"border-radius: 2px\">sklearn</button>\n</div>\n</p>"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 25, 
            "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Downloading Data\nAfter downloading the data file, we will use Pandas <b>read_csv()</b> method to import data into pandas dataframe. "
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 26, 
            "source": "balance_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data',\n                           sep= ',', header= None)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Balance Scale Data Set Description\n\nBalance Scale data set consists of 5 attributes, 4 as feature attributes and 1 as the target attribute. We will try to build a classifier for predicting the Class attribute. The index of target attribute is 1st.\n\n1. Target-Attribute: 3 values (L, B, R)\n2. Left-Weight: 5 values (1, 2, 3, 4, 5)\n3. Left-Distance: 5 values (1, 2, 3, 4, 5)\n4. Right-Weight: 5 values (1, 2, 3, 4, 5)\n5. Right-Distance: 5 values (1, 2, 3, 4, 5)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 1. To view the shape of dataset"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(625, 5)"
                    }, 
                    "execution_count": 27
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 27, 
            "source": "#run code here to view the shape of dataset\nbalance_data.shape"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 2. To view the first five records of dataset"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>R</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   0  1  2  3  4\n0  B  1  1  1  1\n1  R  1  1  1  2\n2  R  1  1  1  3\n3  R  1  1  1  4\n4  R  1  1  1  5"
                    }, 
                    "execution_count": 28
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 28, 
            "source": "#run code here to view what data looks like\nbalance_data.head()"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3. Data Slicing\n\nData slicing is a step to split data into train and test set. Training data set can be used specifically for our model building. Test dataset should not be mixed up while building model. Even during standardization, we should not standardize our test set.\n\nThe below snippet divides data into feature set & target set. \nThe \u201cX \u201d set consists of predictor variables. It consists of data from 2nd column to 5th column. The \u201cY\u201d set consists of the outcome variable. It consists of data in the 1st column. We are using <b>\u201c.values\u201d</b> of numpy converting our dataframes into numpy arrays."
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 29, 
            "source": "\nX = balance_data.values[:,1:5]\nY = balance_data.values[:,0]\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 4. Let\u2019s split our data into training and test set.\n\n<p>We will use sklearn\u2019s <b>train_test_split()</b> method.</p>\n\nThe below snippet will split data into training and test set. X_train, y_train are training data, and X_test, y_test belongs to the test dataset.\n\nThe parameter test_size is given value 0.3; it means test sets will be 30% of whole dataset and training dataset\u2019s size will be 70% of the entire dataset. random_state variable is a pseudo-random number generator state used for random sampling. Here we use random_state = 100."
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 30, 
            "source": "\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 5. Decision Tree Training\n\nNow we fit Decision tree algorithm on training data, predicting labels for validation dataset and printing the accuracy of the model using various parameters.\n\nDecisionTreeClassifier(): This is the classifier function for DecisionTree. It is the main function for implementing the algorithms. Some important parameters are:"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=5, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n            splitter='best')"
                    }, 
                    "execution_count": 31
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 31, 
            "source": "# Decision Tree Classifier with criterion gini index\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5)\nclf_gini.fit(X_train, y_train)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<button type=\"button\" class=\"btn btn-success\" data-toggle=\"collapse\" data-target=\"#info1\">\nClick here for the more information</button>\n<div id=\"info1\" class=\"collapse\">\n<p style=\"color:#339933\"><b>clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5)</b><br>\n<small><small>- Define the model function.</small></small><br>\n<small><small>- The value of parameter <b>criterion</b> inclueds \"gini\" and \"entropy\", which are statistical index to calculate information gain.</small></small><br>\n<small><small>- The value of parameter <b>min_sample_leaf</b> equals to 5 means in one leaf node, the minimum samples number is 5.</small></small><br>\n<b>clf_gini.fit(X_train, y_train)</b><br>\n<small><small>- Model fit, in this step, the data fit in the model, then the tree start to split.</small></small><br>\n</p>\n\n</div>"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=5, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n            splitter='best')"
                    }, 
                    "execution_count": 32
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 32, 
            "source": "# Decidion tree Classifier with criterion information gain(entropy)\nclf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\nclf_entropy.fit(X_train, y_train)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 6. Prediction\n\nNow, we have modeled 2 classifiers. One classifier with gini index and another one with information gain as the criterion. We are ready to predict classes for our test set. We can use <b>predict()</b> method. Let\u2019s try to predict target variable for test set\u2019s 1st record."
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array(['R'], dtype=object)"
                    }, 
                    "execution_count": 33
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 33, 
            "source": "# Let\u2019s try to predict target variable for test set\u2019s 1st record.\n# Recall that the 1st record is [4, 4, 3, 3]\nclf_gini.predict([[4, 4, 3, 3]])"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array(['R', 'L', 'R', 'R', 'R', 'L', 'R', 'L', 'L', 'L', 'R', 'L', 'L',\n       'L', 'R', 'L', 'R', 'L', 'L', 'R', 'L', 'R', 'L', 'L', 'R', 'L',\n       'L', 'L', 'R', 'L', 'L', 'L', 'R', 'L', 'L', 'L', 'L', 'R', 'L',\n       'L', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'R',\n       'L', 'R', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'L', 'L',\n       'L', 'L', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R',\n       'R', 'L', 'R', 'L', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R',\n       'R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'L', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'R', 'R',\n       'R', 'R', 'R', 'L', 'R', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'R',\n       'R', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'L',\n       'R', 'L', 'L', 'R', 'L', 'L', 'R', 'L', 'R', 'L', 'R', 'R', 'R',\n       'L', 'R', 'R', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'R', 'R', 'L',\n       'R', 'R', 'R', 'L', 'R', 'L', 'L', 'L', 'L', 'R', 'R', 'L', 'R',\n       'R', 'L', 'L', 'R', 'R', 'R'], dtype=object)"
                    }, 
                    "execution_count": 34
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 34, 
            "source": "# Prediction for Decision Tree classifier with criterion as gini index\ny_pred = clf_gini.predict(X_test)\ny_pred"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array(['R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'R', 'R', 'R', 'L',\n       'L', 'R', 'L', 'R', 'L', 'L', 'R', 'L', 'R', 'L', 'L', 'R', 'L',\n       'R', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'L', 'L', 'L', 'L', 'R',\n       'L', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'L', 'R', 'L', 'L', 'R',\n       'L', 'L', 'R', 'L', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'R',\n       'L', 'L', 'R', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R',\n       'R', 'L', 'R', 'L', 'L', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R',\n       'R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'L', 'L', 'R',\n       'R', 'R', 'R', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'R', 'R',\n       'L', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'L', 'L', 'L', 'R',\n       'R', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'L',\n       'R', 'L', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'R', 'R', 'R', 'R',\n       'L', 'R', 'R', 'R', 'R', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'L',\n       'R', 'L', 'R', 'L', 'R', 'L', 'L', 'L', 'L', 'L', 'R', 'R', 'R',\n       'L', 'L', 'L', 'R', 'R', 'R'], dtype=object)"
                    }, 
                    "execution_count": 35
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 35, 
            "source": "# Prediction for Decision Tree classifier with criterion as information gain(entropy)\ny_pred_en = clf_entropy.predict(X_test)\ny_pred_en"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 7. Calculating Accuracy Score\n<p>The function <b>accuracy_score()</b> will be used to print accuracy of Decision Tree algorithm. By accuracy, we mean the ratio of the correctly predicted data points to all the predicted data points. Accuracy as a metric helps to understand the effectiveness of our algorithm. It takes 4 parameters.</p>\n<p style = \"color:#cc6600\">\n    y_true,<br>\n    y_pred,<br>\n    normalize,<br>\n    sample_weight.<br>\n</p>\n<p>Out of these 4, normalize & sample_weight are optional parameters. The parameter y_true  accepts an array of correct labels and y_pred takes an array of predicted labels that are returned by the classifier. It returns accuracy as a float value.</p>"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "accuracy is \n"
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "73.40425531914893"
                    }, 
                    "execution_count": 36
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 36, 
            "source": "# Accuracy for Decision Tree classifier with criterion as gini index\nprint ('accuracy is ')\naccuracy_score(y_test,y_pred)*100"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "accuracy is \n"
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "70.744680851063833"
                    }, 
                    "execution_count": 37
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 37, 
            "source": "# Accuracy for Decision Tree classifier with criterion as information gain\nprint ('accuracy is ')\naccuracy_score(y_test,y_pred_en)*100"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 1.6 (Unsupported)", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.6.3", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3"
        }
    }, 
    "nbformat": 4
}